{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import mlflow\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from logger.logger import logging\n",
    "from exception.exception import customexception\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from utils.common import create_directories,read_yaml,load_object,read_csv,save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config_entity -> configuration_manager -> component_file -> pipeline -> main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config_entity\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    trained_model_path: Path\n",
    "    test_data_path: Path\n",
    "    metrics_json_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration_menager\n",
    "from constants import CONFIG_FILE_PATH\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_filepath=CONFIG_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_evaluation_config(self)->ModelEvaluationConfig:\n",
    "        training_config = self.config.model_training\n",
    "        datatransform_config = self.config.data_transformation\n",
    "        model_evaluation_config = self.config.model_evaluation\n",
    "        \n",
    "        create_directories([model_evaluation_config.root_dir])\n",
    "        evaluation_config = ModelEvaluationConfig(\n",
    "            trained_model_path=training_config.model_path,\n",
    "            test_data_path=datatransform_config.transform_data_files,\n",
    "            metrics_json_path=model_evaluation_config.metrics_path\n",
    "        )\n",
    "        return evaluation_config\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component file\n",
    "class ModelEvaluation:\n",
    "    def __init__(self,config:ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def eval_metrics(self,y_pred,y_test):\n",
    "        mse = mean_squared_error(y_test,y_pred)\n",
    "        logging.info(f\"Mean squared error of model is: {mse}\")\n",
    "        mae = mean_absolute_error(y_test,y_pred)\n",
    "        logging.info(f\"Mean absolute error of model is: {mae}\")\n",
    "        score_r2 = r2_score(y_test,y_pred)\n",
    "        logging.info(f\"r2 score of model is: {score_r2}\")\n",
    "        \n",
    "        logging.info(\"Storing metrics into json file\")\n",
    "        metrics = {\"MSE\":mse,\"MAE\":mae,\"r2 score\":score_r2}\n",
    "        json_filepath = Path(self.config.metrics_json_path)\n",
    "        # print(json_filepath)\n",
    "        save_json(json_filepath,metrics)\n",
    "        return mse,mae,score_r2\n",
    "    \n",
    "    def model_evaluate(self):\n",
    "        logging.info(\"Loading the model\")\n",
    "        model_path = self.config.trained_model_path\n",
    "        model_obj = load_object(model_path)\n",
    "        \n",
    "        logging.info(\"Loading the testing data\")\n",
    "        test_data_path = os.path.join(self.config.test_data_path,\"transform_test.csv\")\n",
    "        test_df = read_csv(test_data_path)\n",
    "        \n",
    "        X_test = test_df.drop('price',axis=1)\n",
    "        y_test = test_df['price']\n",
    "        \n",
    "        logging.info(\"Evaluating data\")\n",
    "        y_pred = model_obj.predict(X_test)\n",
    "        \n",
    "        mse,mae,score_r2 = self.eval_metrics(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifacts\\model_evaluation\\metrics.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    logging.info(\"Model evaluation started\")\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(model_evaluation_config)\n",
    "    model_evaluation.model_evaluate()\n",
    "    logging.info(\"Model evaluation completed\")\n",
    "except Exception as e:\n",
    "    raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemprediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
