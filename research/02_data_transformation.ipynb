{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from utils.common import create_directories\n",
    "from dataclasses import dataclass\n",
    "from exception.exception import customexception\n",
    "from logger.logger import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler,OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before start creating this flow update the config/config.yaml file \n",
    "# conifg_entity --> configuration_manager --> component file --> pipeline_file --> main_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config entity\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    unzip_data_files: Path\n",
    "    preprocessed_data_files: Path\n",
    "    transform_data_files: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration manager\n",
    "from constants import CONFIG_FILE_PATH\n",
    "from utils.common import create_directories,read_yaml,read_csv\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,config_filepath=CONFIG_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "        \n",
    "    def get_datatransformation_config(self)->DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            unzip_data_files = config.unzip_data_files,\n",
    "            preprocessed_data_files = config.preprocessed_data_files,\n",
    "            transform_data_files = config.transform_data_files\n",
    "        )\n",
    "        \n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# component creation\n",
    "class DataTransformation:\n",
    "    def __init__(self,config=DataTransformationConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def data_preprocessing(self):\n",
    "        try:\n",
    "            # reading data\n",
    "            dataset_path = self.config.unzip_data_files\n",
    "            dataframe = read_csv(dataset_path)\n",
    "            \n",
    "            logging.info(\"Data preprocessing started\")\n",
    "            \n",
    "            # preprocessing on data\n",
    "            dataframe.drop(\"id\",axis=1,inplace=True)\n",
    "            dataframe.drop(dataframe[(dataframe['price'] >= 10000) & (dataframe['carat'] > 0)  & (dataframe['carat'] < 0.9)].index,inplace=True)\n",
    "            dataframe.drop(dataframe[(dataframe['x']<0.2) | (dataframe['y']<0.2) | (dataframe['z']<0.2)].index,inplace=True)\n",
    "            \n",
    "            # saving file in the data_transformation artifact\n",
    "            preprocessed_data_path = self.config.preprocessed_data_files\n",
    "            create_directories([preprocessed_data_path])\n",
    "            \n",
    "            file_path = os.path.join(preprocessed_data_path,\"preprocessed_data.csv\")\n",
    "            dataframe.to_csv(file_path,index=False)\n",
    "            \n",
    "            \n",
    "            # dependent and independent feature separation\n",
    "            X = dataframe.drop('price',axis=1)\n",
    "            y = dataframe['price'] \n",
    "            \n",
    "            # preprocessing pipeline creation\n",
    "            cat_cols = X.select_dtypes(include=\"object\").columns\n",
    "            nume_cols = X.select_dtypes(exclude=\"object\").columns\n",
    "            \n",
    "            cut_categories = [\"Fair\",\"Good\",\"Very Good\",\"Premium\",\"Ideal\"]\n",
    "            color_categories = [\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\"]\n",
    "            clarity_categories = [\"I1\",\"SI1\",\"SI2\",\"VS2\",\"VS1\",\"VVS2\",\"VVS1\",\"IF\"]\n",
    "            \n",
    "            numeric_pipeline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                    ('scaler',StandardScaler())\n",
    "                ]\n",
    "            )\n",
    "            categorical_pipline = Pipeline(\n",
    "                steps=[\n",
    "                    ('imputer',SimpleImputer(strategy='most_frequent')),\n",
    "                    ('ordinalencoder',OrdinalEncoder(categories=[cut_categories,color_categories,clarity_categories]))\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # use column transformer to join the two pipelines\n",
    "            # (name, transformer, columns)\n",
    "            preprocessor = ColumnTransformer([\n",
    "                (\"Numeric Pipeline\",numeric_pipeline,nume_cols),\n",
    "                (\"Categorical Pipeline\",categorical_pipline,cat_cols)\n",
    "            ])\n",
    "            logging.info(\"Data preprocessing completed, preprocessing object return successfully\")\n",
    "            \n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise customexception(e,sys)\n",
    "    \n",
    "    def data_transformation(self):\n",
    "        try: \n",
    "            preprocessor_obj = self.data_preprocessing()\n",
    "            \n",
    "            logging.info(\"Data transformation started\")\n",
    "            \n",
    "            create_directories([self.config.transform_data_files])\n",
    "            \n",
    "            dataset_path = os.path.join(self.config.preprocessed_data_files,\"preprocessed_data.csv\")\n",
    "            dataframe = read_csv(dataset_path)\n",
    "            X = dataframe.drop('price',axis=1)\n",
    "            y = dataframe['price']\n",
    "            \n",
    "            X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=42)\n",
    "            \n",
    "            X_train = preprocessor_obj.fit_transform(X_train)\n",
    "            X_test = preprocessor_obj.transform(X_test)\n",
    "            \n",
    "            #dataframe creation\n",
    "            # X_train = pd.DataFrame(data=X_train,columns=preprocessor_obj.get_feature_names_out())\n",
    "            # y_train = pd.DataFrame(data=y_train,columns=['price'])\n",
    "            # train_df = pd.concat([X_train,y_train],axis=1)\n",
    "            # train_df_path = os.path.join(self.config.transform_data_files,\"transform_train.csv\")\n",
    "            # train_df.to_csv(train_df_path,index=False)\n",
    "            \n",
    "            columns = list(dataframe.columns)\n",
    "            print(columns)\n",
    "            \n",
    "            train_arr = np.c_[X_train,np.array(y_train)]\n",
    "            train_df = pd.DataFrame(train_arr,columns=columns)\n",
    "            train_df_path = os.path.join(self.config.transform_data_files,\"transform_train.csv\")\n",
    "            train_df.to_csv(train_df_path,index=False)\n",
    "            \n",
    "            # X_test = pd.DataFrame(data=X_test,columns=preprocessor_obj.get_feature_names_out())\n",
    "            # y_test = pd.DataFrame(data=y_test,columns=['price'])\n",
    "            # test_df = pd.concat([X_test,y_test],axis=1)\n",
    "            # test_df_path = os.path.join(self.config.transform_data_files,\"transform_test.csv\")\n",
    "            # test_df.to_csv(test_df_path,index=False)\n",
    "            test_arr = np.c_[X_test,np.array(y_test)]\n",
    "            test_df = pd.DataFrame(test_arr,columns=columns)\n",
    "            test_df_path = os.path.join(self.config.transform_data_files,\"transform_test.csv\")\n",
    "            test_df.to_csv(test_df_path,index=False)\n",
    "            \n",
    "\n",
    "            logging.info(\"Data transformation completed\")\n",
    "        except Exception as e:\n",
    "            raise customexception(e,sys)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_datatransformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.data_transformation()\n",
    "except Exception as e:\n",
    "    raise customexception(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['carat', 'cut', 'color', 'clarity', 'depth', 'table', 'x', 'y', 'z', 'price']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "columns = list(df.columns)\n",
    "columns.remove('id')\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemprediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
